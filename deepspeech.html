<html>
<head><title>Installing and training DeepSpeech</title>
</head>
<body>

<h3>Download DeepSpeech code:</h3>

<pre>
$ git clone ##depth 1 https://github.com/mozilla/DeepSpeech.git
</pre>

<h3>Setup a virtualenv:</h3>

<pre>
$ virtualenv -p python3 $HOME/tmp/deepspeech-venv/

$ source $HOME/tmp/deepspeech-venv/bin/activate
</pre>

<h3>Install DeepSpeech python bindings:</h3>
<pre>
$ pip3 install deepspeech
$ cd native_client 
$ python3 ../util/taskcluster.py ##target .
$ cd ..
</pre>

<h3>Install a tonne of requirements for training</h3>
<pre>
$ pip3 install -r requirements.txt
</pre>


<h3>Install kenlm:</h3>
<pre>
$ wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz
$ sudo apt-get install libboost-program-options1.63-dev libboost-system1.63-dev libboost-thread1.63-dev libboost-test1.63-dev libeigen3-dev
$ mkdir kenlm/build
$ cd kenlm/build
$ cmake ..
$ make -j2
</pre>

<h3>Setup the data</h3>
<pre>
$ mkdir data/chv_digits
$ python3 bin/import_digits.py ~/source/Turkic_TTS/corpus/chv/speakers/digits/ chv
$ mv vocab.txt alphabet.txt chv_* data/chv_digits
$ cd data/chv_digits
$ cat vocab.txt | ~/source/kenlm/build/bin/lmplz ##discount_fallback -o 3 ##arpa vocab.arpa
$ ~/source/kenlm/build/bin/build_binary -T -s vocab.arpa lm.binary
$ ../../native_client/generate_trie alphabet.txt lm.binary vocab.txt trie
$ wc -l *
   23 alphabet.txt
    7 chv_digits.dev.csv
    7 chv_digits.test.csv
   49 chv_digits.train.csv
   31 lm.binary
 1004 trie
  557 vocab.arpa
   48 vocab.txt
 1726 total
$ cd ../../
$ mkdir models
</pre>

<h3> Train the model </h3>
<pre>
$ python3 DeepSpeech.py ##train_files data/chv_digits/chv_digits.train.csv ##dev_files data/chv_digits/chv_digits.dev.csv ##test_files data/chv_digits/chv_digits.test.csv ##alphabet_config_path data/chv_digits/alphabet.txt ##lm_binary_path data/chv_digits/lm.binary ##lm_trie_path data/chv_digits/trie ##validation_step 1 ##train_batch_size 5 ##test_batch_size 5 ##dev_batch_size 5 ##early_stop True ##export_dir models/digits ##epoch 8 ##report_count 100 ##n_hidden 494 ##learning_rate 0.00095  ##dropout_rate 0.22 ##max_to_keep 2
</pre>

</body>


</html>
<!--

python -u DeepSpeech.py \
  ##train_files /home/nvidia/DeepSpeech/data/alfred/train/train.csv \
  ##dev_files /home/nvidia/DeepSpeech/data/alfred/dev/dev.csv \
  ##test_files /home/nvidia/DeepSpeech/data/alfred/test/test.csv \
  ##train_batch_size 90 \
  ##dev_batch_size 80 \
  ##test_batch_size 70 \
  ##n_hidden 375 \
  ##epoch 400 \
  ##validation_step 1 \
  ##early_stop True \  # early stop activated
  ##earlystop_nsteps 8 \ # if validation stop doesn't shut down after 8, stop
  ##estop_mean_thresh 0.001 \ # thin params for early stop nsteps, to knowthe loss position
  ##estop_std_thresh 0.001 \ # same
  ##dropout_rate 0.012 \ # a param to avoid overfitting. (works for me)
  ##learning_rate 0.001 \ # learning speed (too small = too long ; to high = we'll perhaps miss the best loss point value
  ##beam_width 1024 \ # number of probabilities to keep in memory, to choose the best result (hughter = more memory
  ##lm_weight 5 \ # params for LM/trie integration in model creation
  ##word_count_weight 1.0 \ # same
  ##valid_word_count_weight 1.0 \ same
  ##export_dir /home/nvidia/DeepSpeech/data/alfred/results/model_export/ \
  ##checkpoint_dir /home/nvidia/DeepSpeech/data/alfred/results/checkout/ \
  ##decoder_library_path /home/nvidia/tensorflow/bazel-bin/native_client/libctc_decoder_with_kenlm.so \
  ##alphabet_config_path /home/nvidia/DeepSpeech/data/alphabet.txt \
  ##lm_binary_path /home/nvidia/DeepSpeech/data/lm.binary \
  ##lm_trie_path /home/nvidia/DeepSpeech/data/trie \
  "$@"

-->
