<html>
<head>
<title>Practical: Segmentation and tokenisation</title>
</head>
<body>

<h2>Segmentation</h2>
<p>
Extract 50 paragraphs of text at random from a Wikipedia dump of a language of your choice and compare two sentence segmenters, 
for example the <a href="https://github.com/diasks2/pragmatic_segmenter">pragmatic segmenter</a>
and NLTK's <a href="https://kite.com/python/docs/nltk.tokenize.punkt">Punkt</a>. You may also compare either of those against
a segmenter that you have written yourself.
</p>
<p>
You will need the file called <tt>-pages-articles.xml.bz2</tt>. You can find it on the <a href="http://dumps.wikimedia.org">WikiMedia
dumps site</a>. To extract the text, you can use WikiExtractor. Then use the segmenters to segment the raw text output
into sentences.
</p>
<p>
The comparison should include: 
<ul>
  <li>Quantitative evaluation: Accuracy percentage (how many sentence boundaries were detected correctly)</li>
  <li>Qualitative evaluation: What kind of mistakes does each segmenter make?</li>
</ul>
</p>
<p>
To install pragmatic_segmenter, do the following:

<pre>
$ git clone https://github.com/diasks2/pragmatic_segmenter
</pre>

This will check out the code.

<pre>
$ cd pragmatic_segmenter/lib
</pre>

You will need to put this code into a file, call it <tt>segmenter.rb</tt>:

<pre>
require 'pragmatic_segmenter'

lang = "en"
if ARGV[0]
    lang=ARGV[0]
end

STDIN.each_with_index do |line, idx|
    ps = PragmaticSegmenter::Segmenter.new(text: line, language: lang)
    ps.segment
    for i in ps.segment
        print(i,"\n")
    end
end
</pre>

You can then run it like this:

<pre>
$ echo "This is a test.This is a test. This is a test." | ruby -I . segmenter.rb 
This is a test.
This is a test.
This is a test.
</pre>

If you get errors like <tt>cannot load such file -- unicode (LoadError)</tt>, then you
need to install <tt>ruby-unicode</tt>, on apt-based systems, you can use:

<pre>
$ sudo apt-get install ruby-unicode
</pre>

</p>


<h2>Tokenisation</h2>
<p>
First download the UD treebank for Japanese (<tt>UD_Japanese-GSD</tt>) from the <a href="http://github.com/UniversalDependencies/">Universal dependencies</a> GitHub repository.
Then implement the left-to-right longest-match algorithm (also known as <tt>maxmatch</tt>). For a description of the 
algorithm see Section 3.9.1 in Jurafsky and Martin. 
</p>
<p>
<ul>
<li>Extract a dictionary of segmented surface forms from <tt>UD_Japanese-GSD/ja_gsd-ud-train.conllu</tt>. The dictionary should contain 15,326 forms.</li>
<li>Test how well the left-to-right longest match algorithm is capable of segmenting the text in <tt>UD_Japanese-GSD/ja_gsd-ud-test.conllu</tt> To perform the evaluation, you can use some Word Error Rate calculating code that you find online, e.g. 
<ul>
  <li><a href="https://github.com/zszyellow/WER-in-python">WER in python</a></li>
  <li><a href="http://svn.code.sf.net/p/apertium/svn/trunk/apertium-eval-translator/">apertium-eval-translator</a></li>
</ul> 
</li>
</ul>
</p>
<p>
If you have time, test the algorithm with other treebanks for languages which do not use word separators, e.g. Chinese, Thai.
</p>

</body>
</html>
