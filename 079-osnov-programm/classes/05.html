<html>
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" >
  <link rel="stylesheet" type="text/css" href="../style.css" />
  <title>Class 5</title>

</head>
<body>
<div class="page">

<h2> Class 5 </h2>


<h3>Regular expressions</h3>

You got a bit familiar with regular expressions in the <a href="01.html">tutorial</a> on the command line with <tt>sed</tt> and
<tt>grep</tt>. If you enjoyed that, you will be pleasantly surprised that it's also possible to use
regular expressions in Python! There is a nice tutorial on the Python site <a href="https://docs.python.org/3/library/re.html">here</a>,
but here are the highlights:

You can find a matching part of a string using <tt>search()</tt>:

<pre>
>>> import re 

>>> s = 'Привет мир!'

>>> re.search(r'м[а-я]+', s)
</pre>

If you want to replace parts of a string, you can use <tt>sub()</tt> which works a bit like <tt>sed</tt>:

<pre>
>>> re.sub('[а-я]', 'x', s)
'Пxxxxx xxx!'
</pre>

<h3>Libraries and modules in Python</h3>

<div style="background-color:red; width:100%;text-align:center"><span style="color: white"><b>TODO: Expand</b></span></div>

<h4><tt>matplotlib</tt></h4>

One really cool library in Python is <a href="https://matplotlib.org"><tt>matplotlib</tt></a>, this
is a library for visualising data and plotting graphs.

In your virtual machine you can install it as follows:

<pre>
$ sudo apt-get install python3-matplotlib
</pre>

Now you can try making a small program to take the frequency/rank list that you had earlier 
and plot it on a graph.

First make sure that you have the ranks file:

<pre>
$ head ranks.txt 
1	11324	.
2	11221	,
3	3972	æмæ
4	3121	:
5	2745	)
6	2732	(
7	2616	у
8	1708	азы
9	1646	йæ
10	1301	"
</pre>

Then try this program:

<pre>
import sys
import matplotlib.pyplot as plt

x = []
y = []

fd = open('ranks.txt', 'r')
for line in fd.readlines():
        line = line.strip()
        if line == '':
                continue
	
        row = line.split('\t')
        x.append(int(row[0]))
        y.append(int(row[1]))

plt.plot(x, y, 'ro')
plt.show()
</pre>

<!--<h4> <tt>numpy</tt> </h4> -->
<h4> ElementTree </h4> 

<a href="https://docs.python.org/3/library/xml.etree.elementtree.html">ElementTree</a> is a Python module for parsing XML. XML is a file format that is widely 
used for storing structured data. The <a href="http://depts.washington.edu/uwcl/odin/">ODIN
project</a> is a project which aims to collect and encode interlinear glossed text
in very many languages. You can find an example file for Icelandic
<a href="http://depts.washington.edu/uwcl/odin/isl-ex.xml">here</a>.

There is a lot of data there, but let's say we just wanted to pull out part of it, for 
example the normalised gloss. First download the file:

<pre>
$ wget "http://depts.washington.edu/uwcl/odin/isl-ex.xml"
</pre>

One way to get the information would be to just do string processing, but a better way
is to use the structure and an XML parser. The first thing to do is to import
ElementTree and use the <tt>parse()</tt> function to parse the XML.

<pre>
import xml.etree.ElementTree as ET

tree = ET.parse('isl-ex.xml')
</pre>

The first thing we can do is look at the tag of the root node in the tree:

<pre>
root = tree.getroot()

print(root.tag)
</pre>

We can also pull out nodes in the tree that have a specific 

<!--<tier id="n" type="odin" alignment="c" state="normalized">-->



<!--<h4> </h4> -->

<h3>Screenscraping</h3>

Screenscraping (or web scraping) is the task of pulling human-readable data from the web and turning it 
into machine-readable form. There are two main components, the first is <em>crawling</em> which means
downloading the pages you are interested in. The second is <em>scraping</em> which means getting the 
information out of those pages. We're going to flip things around and start with the second first.

Run this command:

<pre>
$ wget -q -O - "http://ru.wiktionary.org/wiki/страх" 
</pre>

The output is pretty terrifying right ? A tonne of HTML code, very difficult to find the structure or data. But if 
we go to the page <a href="http://ru.wiktionary.org/wiki/страх">страх</a> there is a lot of useful data. What kind
of data might we be interested in ? 

<ul>
<li> Stem </li>
<li> Inflection table </li>
<li> Zaliznjak code </li>
<li> Definitions </li>
<li> Translations </li>
<li> Pronunciation </li>
<li> ... </li>
</ul>

Let's start with a simple exercise to extract the stem, the pronunciation and the Zaliznjak code. Download
the page and save it to a file called <tt>страх.html</tt> with <tt>wget</tt> or <tt>curl</tt>. Now look
through the HTML and find the lines where the information you are looking for is.

Here is the line with the stem,

<pre>
&lt;p&gt;Корень: &lt;b&gt;-страх-&lt;/b&gt;.&lt;/p&gt;
</pre>

Here is the line with the pronunciation,

<pre>
&lt;li&gt;&lt;a href="https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D0%B6%D0%B4%D1%83%D0%BD%D0%B0%D1%80%D0%BE%D0%B4%D0%BD%D1%8B%D0%B9_%D1%84%D0%BE%D0%BD%D0%B5%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B0%D0%BB%D1%84%D0%B0%D0%B2%D0%B8%D1%82" class="extiw" title="w:Международный фонетический алфавит"&gt;МФА&lt;/a&gt;: ед.&amp;#160;ч.&amp;#160;[&lt;span class="IPA" style="white-space: nowrap;"&gt;strax&lt;/span&gt;]
</pre>

And here is the line with the Zaliznjak code,

<pre>
&lt;p&gt;&lt;a href="/wiki/%D1%81%D1%83%D1%89%D0%B5%D1%81%D1%82%D0%B2%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D0%BE%D0%B5" title="существительное"&gt;Существительное&lt;/a&gt;, неодушевлённое, мужской род, 2-е &lt;a href="/wiki/%D1%81%D0%BA%D0%BB%D0%BE%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5" title="склонение"&gt;склонение&lt;/a&gt; (тип склонения 3a по &lt;a href="/wiki/%D0%92%D0%B8%D0%BA%D0%B8%D1%81%D0%BB%D0%BE%D0%B2%D0%B0%D1%80%D1%8C:%D0%98%D1%81%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D1%81%D0%BB%D0%BE%D0%B2%D0%B0%D1%80%D1%8F_%D0%97%D0%B0%D0%BB%D0%B8%D0%B7%D0%BD%D1%8F%D0%BA%D0%B0" title="Викисловарь:Использование словаря Зализняка"&gt;классификации А.&amp;#160;А.&amp;#160;Зализняка&lt;/a&gt;).&lt;/p&gt;
</pre>

We might decide that all of this would be a lot easier to process if we didn't have the HTML code in the 
way... that might not always be the case. But we could try a simple function like the following 
to get rid of (most of) the HTML:

<pre>
def strip_html(h):
	o = ''
	inTag = False
	for c in h: 
		if c == '<':
			inTag = True
			continue
		if c == '>':
			inTag = False
			continue
		if not inTag:
			o = o + c
	return o	
</pre>

Now let's see what we get:

<pre>
Корень: -страх-.
</pre>

<pre>
МФА: ед.&amp;#160;ч.&amp;#160;[strax]
</pre>

<pre>
Существительное, неодушевлённое, мужской род, 2-е склонение (тип склонения 3a по классификации А.&amp;#160;А.&amp;#160;Зализняка).
</pre>

Ok, this is clearer, let's come up with some heuristics:

<ul>
  <li> The stem is found on the line that starts with 'Корень' and appears after the ':' and
before the '.'</li>
  <li> The pronunciation is found on the line that contains 'МФА' and comes between '[' and ']'. </li>
  <li> The declension type is on the line that contains 'тип склонения' and appears immediately after that.</li>
</ul>

If we read through the file line-by-line we can use these heuristics to identify where the information
is that we want to extract and how to extract it.

Take a look at the following code:

<pre>
tem = '_'
zkod = '_'
ipa = '_'
for line in sys.stdin.readlines(): 
        line = line.strip()
        text = strip_html(line);

        if text.count('Корень:') > 0:
                stem = text.split(':')[1].strip('.')
        if text.count('МФА') > 0:
                ipa = text.split('[')[1].split(']')[0]
        if text.count('тип склонения') > 0:
                zkod = text.split('тип склонения')[1].strip().split(' ')[0]

        if stem != '_' and zkod != '_' and ipa != '_':
                print('%s\t%s\t%s' % (stem, zkod, ipa))
                stem = '_'
                zkod = '_'
                ipa = '_'
</pre>

And try running it on your <tt>страх.html</tt> file. You should get output something like:

<pre>
$ cat страх.html | python3 wiktionary.py
 -страх-	3a	strax
</pre>

Great! But there is a chance our code is a bit fragile, so we should try it with a 
few more pages. Let's try it with the page <a href="https://ru.wiktionary.org/wiki/%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D0%BE">дерево</a>.

<pre>
$ cat дерево.html | python3 wiktionary.py 
 -дерев-; окончание	1a^	ˈdʲerʲɪvə
 --	по	ˈd̪ɛ.rɛ̝.wɔ
</pre>

Uh oh, this isn't exactly what we were looking for. There are two main issues:

<ul>
  <li> It also picks up the information from the Ukrainian section of the page. </li>
  <li> In this particular page there is more information regarding the stem, you also get the ending.</li>
</ul>

So how can we fix our code... Let's look at the HTML again. 

<pre>
&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h1&gt;&lt;span class="mw-headline" id=".D0.A0.D1.83.D1.81.D1.81.D0.BA.D0.B8.D0.B9"&gt;Русский&lt;/span&gt;&lt;/h1&gt;
&lt;h3&gt;&lt;span class="mw-headline" id=".D0.9C.D0.BE.D1.80.D1.84.D0.BE.D0.BB.D0.BE.D0.B3.D0.B8.D1.87.D0.B5.D1.81.D0.BA.D0.B8.D0.B5_.D0.B8_.D1.81.D0.B8.D0.BD.D1.82.D0.B0.D0.BA.D1.81.D0.B8.D1.87.D0.B5.D1.81.D0.BA.D0.B8.D0.B5_.D1.81.D0.B2.D0.BE.D0.B9.D1.81.D1.82.D0.B2.D0.B0"&gt;Морфологические и синтаксические свойства&lt;/span&gt;&lt;/h3&gt;
</pre>

So, the sections on the page are implemented with the <tt>&lt;h1&gt;</tt> tag in HTML. We can use a 
variable to track what heading we're in and skip the section if it isn't Russian. For example:

<pre>
h1 = '_';
for line in sys.stdin.readlines(): 
	if line.count('&lt;h1&gt;') > 0: 
		h1 = strip_html(line)

	if h1 != 'Русский': 
		continue

	...
</pre>

I leave the second problem (dealing with the <em>окончание</em>) as an exercise for the reader.

<h3>Unigram tagger</h3>

<p>
A unigram tagger works by assiging the most frequent tag seen for a given
surface form. It may additionally <em>back-off</em> to the most frequent
tag in general. Our unigram language model has all of this information, we 
just need to apply it to our input stream.
</p>

<p>
The first thing your program will need to do is load the language model and 
calculate the most frequent tag per word and most frequent tag in general. You
can use a <tt>dict</tt> for the word → tag mapping and a simple variable
for the most frequent tag.
</p>

Example input:

<pre>
# sent_id = 482
# text = Часть его положений отменена Конституционным судом в 2013 году.
1	Часть	_	_	_	_	_	_	_	_
2	его	_	_	_	_	_	_	_	_
3	положений	_	_	_	_	_	_	_	_
4	отменена	_	_	_	_	_	_	_	_
5	Конституционным	_	_	_	_	_	_	_	_
6	судом	_	_	_	_	_	_	_	_
7	в	_	_	_	_	_	_	_	_
8	2013	_	_	_	_	_	_	_	_
9	году	_	_	_	_	_	_	_	_
10	.	_	_	_	_	_	_	_	_

</pre>

Output:

<pre>
# sent_id = 482
# text = Часть его положений отменена Конституционным судом в 2013 году.
1	Часть	_	NOUN	_	_	_	_	_	_
2	его	_	NOUN	_	_	_	_	_	_
3	положений	_	NOUN	_	_	_	_	_	_
4	отменена	_	NOUN	_	_	_	_	_	_
5	Конституционным	_	NOUN	_	_	_	_	_	_
6	судом	_	NOUN	_	_	_	_	_	_
7	в	_	ADP	_	_	_	_	_	_
8	2013	_	NOUN	_	_	_	_	_	_
9	году	_	NOUN	_	_	_	_	_	_
10	.	_	PUNCT	_	_	_	_	_	_
</pre> <!-- 70% correct -->

The program should be called <tt><b>tagger.py</b></tt>

<h3> Questions </h3>

<ul>
  <li> How accurate is the tagger ?</li>
  <li> How could you improve performance without incorporating context ...</li>
  <ul>
    <li> using Python string functions ? </li>
    <li> using regular expressions ? </li>
    <li> using screen scraping ? </li>
  </ul>
  <li> Could you store other single-word features in your unigram model ? Which features might you like to store ?</li>
</ul>

<h3> Further reading </h3>

<ul>
  <li> <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup</a> is a nice Python
    library for pulling data out of HTML. It is a more advanced technique than the one I have described here.</li>
</ul>
</div>

</body>
</html>


